The system prompt is designed to make the legal document assistant behave like a careful professional rather than a template generator. Instead of immediately drafting a contract, it follows a controlled workflow: gather details, ask clarifying questions, verify required fields, and only proceed once the information is complete. Placeholders are explicitly disallowed, preventing vague or half-filled agreements. Each document type defines its required inputs upfront—for instance, NDAs require the parties, effective date, purpose, term, and governing jurisdiction; employment agreements need role, start date, compensation, and more. This structure forces the assistant to slow down, reduce ambiguity, and maintain a consistent legal tone. The prompt also defines edge-case rules and a six-step conversation structure that guides the assistant from the initial greeting to document revisions.

The function architecture mirrors the workflow of real legal professionals: collect information, generate a draft, then refine it. The extract_information function builds a session-level state dictionary that updates as the user naturally provides details. Once all necessary fields are captured, generate_document assembles a complete agreement using templates that include standard clauses such as confidentiality obligations, term definitions, material-return rules, governing law, and signature blocks. The apply_edits function then allows users to request revisions in plain language. It uses pattern matching to update specific fields—names, dates, terms, and so on—without regenerating the entire document. Separating these responsibilities keeps the system modular and easier to maintain.

Streaming introduced several technical challenges. Because the model can decide mid-stream to call a function, the system has to parse partial JSON fragments token by token. To handle this, I added an accumulator that rebuilds arguments as they arrive, plus error handling for malformed JSON, missing fields, and unexpected model outputs. Each error is wrapped and emitted through SSE without breaking the stream.

The frontend listens to multiple SSE event types—content, function_call, function_result, document, and error—while keeping the display smooth. Tool messages remain invisible to the user but stay in conversation history for accurate model context. The UI also separates “streaming text in progress” from finalized messages, preventing flicker or duplication.

The project isn’t fully finished yet—there are still some issues with the editing workflow and occasional inconsistencies in how certain edge cases stream. However, the core system is in place, the major functionality works, and the remaining bugs are mostly refinement and reliability issues.