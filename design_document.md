The system prompt is designed to make the legal document assistant feel more like a careful professional than a template generator. Instead of jumping straight into drafting, the AI is intructed to guide users through the process, ask clarifying questions, and only produce a document once all required information is collected. It also refuses to use placeholders, which helps prevent vague or incomplete agreements. The prompt outlines exactly what information is needed for each document type—for example, an NDA must include the disclosing party, receiving party, effective date, purpose, term, and jurisdiction. Employment agreements need details like the employee’s name, role, start date, and salary. By setting these expectations upfront, the prompt forces the assistant to slow down, confirm details, and maintain a professional, consistent tone. It also includes rules for handling edge cases and structures the conversation into six clear steps, guiding the assistant from initial greeting through revision and finalization.

The function design mirrors how real legal professionals work: gather information, produce a draft, and refine it. The extract_information function supports an ongoing back-and-forth by collecting details as the user naturally shares them. It updates a session-level data dictionary so the assistant continuously builds a complete picture. Once enough information is available, the generate_document function creates a full legal document using templates tailored to each document type. For example, the NDA template includes all standard clauses—confidentiality obligations, term definitions, return-of-material rules, governing law, and signature sections. Finally, the apply_edits function lets users request revisions in plain language. It uses pattern matching to update names, dates, terms, and other details without rebuilding the document from scratch. This three-function setup keeps responsibilities cleanly separated, making the system modular and easy to maintain.

Building streaming into this setup introduced several technical challenges. Because the model can decide mid-stream to call a function, the system has to pause generation, run the function, send its result back, and then resume streaming. To make this work, an accumulator collects the function name and arguments as they drip in token by token, since the API streams JSON fragments rather than full objects. Error handling was another major challenge—problems can appear while parsing arguments, executing a function, or producing follow-up messages. Each error must be caught and returned through the SSE stream without breaking the connection. Conversation history also has to stay properly formatted, with function calls and results stored in the correct order so later messages have full context. On the frontend, the UI must handle multiple event types—content, function calls, results, documents, and errors—while showing smooth real-time updates. This required separating streaming text from finalized messages and filtering tool messages out of the visible chat while keeping them in memory for context.
